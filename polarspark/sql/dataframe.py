import polars as pl
from typing import List, Union, Any, Tuple
from .column import Column

class DataFrame:
    def __init__(self, df: pl.DataFrame):
        self._df = df

    def __getitem__(self, item: str) -> Column:
        if isinstance(item, str):
            c = Column(pl.col(item))
            # Store a reference to the df to help with ambiguous joins
            c._df_ref = id(self)
            return c
        raise ValueError(f"Unsupported item type: {type(item)}")

    def __getattr__(self, name: str) -> Column:
        if name in self.columns:
            return self[name]
        raise AttributeError(f"'DataFrame' object has no attribute '{name}'")

    def select(self, *cols: Union[str, Column, List[Union[str, Column]]]) -> "DataFrame":

        normalized_cols = []
        for c in cols:
            if isinstance(c, list):
                normalized_cols.extend(c)
            else:
                normalized_cols.append(c)
        
        exprs = []
        generator_cols = []
        for c in normalized_cols:
            if isinstance(c, str):
                exprs.append(pl.col(c))
            elif isinstance(c, Column):
                exprs.append(c._expr)
                if hasattr(c, "_is_generator") and c._is_generator:
                    # In Polars, we'll need to unnest this column later.
                    # We can't easily know the output name here if it's not aliased.
                    # But we can try to guess it.
                    pass
            else:
                raise ValueError(f"Unsupported column type: {type(c)}")
        
        df = self._df.select(exprs)
        
        # If any was a generator, we might need to unnest.
        # This is a bit hacky. Let's look for struct columns and unnest them if they look like they came from generators.
        # Actually, let's just use unnest if the resulting df has struct columns that were just created?
        # A more reliable way is to check the Column objects we just used.
        for c in normalized_cols:
            if isinstance(c, Column) and hasattr(c, "_is_generator") and c._is_generator:
                # Find the name in the resulting df
                # This is tricky without knowing the exact output name.
                # For posexplode in my test, it's unnamed or autogenerated.
                # Let's unnest ALL columns that are structs for now? No.
                # Let's try to unnest by checking the dtypes.
                struct_cols = [name for name, dtype in zip(df.columns, df.dtypes) if dtype == pl.Struct]
                if struct_cols:
                    df = df.unnest(struct_cols)
        
        return DataFrame(df)


    def filter(self, condition: Union[str, Column]) -> "DataFrame":
        if isinstance(condition, str):
            # Polars' query engine doesn't exactly match Spark's SQL strings, 
            # but for simple cases it might work or we might need a parser.
            # Keeping it simple and assuming it's a column name or we can't support complex SQL yet.
            raise NotImplementedError("String conditions in filter are not yet supported. Use Column expressions.")
        
        return DataFrame(self._df.filter(condition._expr))

    def where(self, condition: Union[str, Column]) -> "DataFrame":
        return self.filter(condition)

    def withColumn(self, name: str, col: Column) -> "DataFrame":
        return DataFrame(self._df.with_columns(col._expr.alias(name)))

    def withColumnRenamed(self, existing: str, new: str) -> "DataFrame":
        return DataFrame(self._df.rename({existing: new}))

    def distinct(self) -> "DataFrame":
        return DataFrame(self._df.unique())

    def drop(self, *cols: str) -> "DataFrame":

        return DataFrame(self._df.drop(list(cols)))

    def limit(self, n: int) -> "DataFrame":
        return DataFrame(self._df.limit(n))

    def orderBy(self, *cols: Union[str, Column], ascending: Union[bool, List[bool]] = True) -> "DataFrame":
        exprs = []
        # Polars handles lists for descending
        is_descending = not ascending if isinstance(ascending, bool) else [not a for a in ascending]
        
        sort_cols = []
        for c in cols:
            if isinstance(c, str):
                sort_cols.append(c)
            elif isinstance(c, Column):
                sort_cols.append(c._expr)
        
        return DataFrame(self._df.sort(sort_cols, descending=is_descending))

    def groupBy(self, *cols: Union[str, Column]) -> "GroupedData":
        normalized_cols = []
        for c in cols:
            if isinstance(c, str):
                normalized_cols.append(c)
            elif isinstance(c, Column):
                normalized_cols.append(c._expr)
        return GroupedData(self._df.group_by(normalized_cols))

    def join(self, other: "DataFrame", on: Union[str, List[str], Column], how: str = "inner") -> "DataFrame":
        # Normalize join type literals
        h = how.lower().replace("_", "")
        how_map = {
            "inner": "inner",
            "left": "left",
            "leftouter": "left",
            "right": "right",
            "rightouter": "right",
            "full": "full",
            "outer": "full",
            "fullouter": "full",
            "semi": "semi",
            "leftsemi": "semi",
            "anti": "anti",
            "leftanti": "anti",
            "cross": "cross"
        }
        pl_how = how_map.get(h, "inner")

        
        if isinstance(on, Column):
            if pl_how == "inner":
                return DataFrame(self._df.join_where(other._df, on._expr))
            else:
                raise NotImplementedError(f"Join on Column expressions with '{how}' join is not yet supported. Only 'inner' is supported for expressions.")
        
        # Resolve Column references in list
        if isinstance(on, list):
            on = [c._expr if isinstance(c, Column) else c for c in on]
            # If they are just pl.col("name"), and coalesce=False is needed?
            # Spark joins on list of names usually coalesces.
            # Polars join(on=["a"]) also coalesces.
            
        return DataFrame(self._df.join(other._df, on=on, how=pl_how))



    def crossJoin(self, other: "DataFrame") -> "DataFrame":
        return DataFrame(self._df.join(other._df, how="cross"))

    def union(self, other: "DataFrame") -> "DataFrame":
        return DataFrame(pl.concat([self._df, other._df]))

    def unionByName(self, other: "DataFrame", allowMissingColumns: bool = False) -> "DataFrame":
        # Polars concat handles names by default if how='vertical'
        return DataFrame(pl.concat([self._df, other._df], how="vertical"))

    def show(self, n: int = 20, truncate: bool = True):




        print(self._df.head(n))

    def collect(self) -> List[Any]:
        return self._df.to_dicts()

    def count(self) -> int:
        return self._df.height

    @property
    def columns(self) -> List[str]:
        return self._df.columns

    def toPandas(self):
        return self._df.to_pandas()

class GroupedData:
    def __init__(self, pl_groupby):
        self._gb = pl_groupby

    def agg(self, *exprs: Union[Column, List[Column]]) -> DataFrame:
        normalized_exprs = []
        for e in exprs:
            if isinstance(e, list):
                normalized_exprs.extend([i._expr for i in e])
            else:
                normalized_exprs.append(e._expr)
        
        return DataFrame(self._gb.agg(normalized_exprs))

    def count(self) -> DataFrame:
        return DataFrame(self._gb.count())

    def max(self) -> DataFrame:
        return DataFrame(self._gb.max())

    def min(self) -> DataFrame:
        return DataFrame(self._gb.min())

    def avg(self) -> DataFrame:
        return DataFrame(self._gb.mean())

    def sum(self) -> DataFrame:
        return DataFrame(self._gb.sum())

